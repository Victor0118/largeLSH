% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{color}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage[linesnumbered,ruled]{algorithm2e}

%\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\DeclareMathOperator{\sign}{sign}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\newtheorem{theorem}{Theorem}[section]


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf LargeLSH: An Exploration of Locality Sensitive 
Hashing for Approximate Nearest Neighbours on High Dimensional Data Using Apache 
Spark}

%for single author (just remove % characters)
\author{
{\rm Wei\ Yang}\\
University of Waterloo
\and
{\rm Zhucheng Tu}\\
University of Waterloo
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}

Nearest neighbour algorithms are a popular class of algorithms in machine learning 
that has a wide range of applications. When the dimensionality of the data is 
small, data structures from computational geometry can be used  
for efficiently finding nearest neighbours. However, when the dimensionality of the 
data is large, the space requirement of these data structures hinder them from 
being used in practice. Today, many internet companies store high dimensional 
features for their business entities, and it is not reasonable to assume all the 
data are stored on the same machine. We explore the problem of approximate nearest 
neighbour search in a distributed setting, specifically looking at the 
effectiveness of locality sensitive hashing (LSH) for this problem on high 
dimensional data. We find that \red{ipsum...}


\section{Introduction}

The problem of finding nearest neighbours has important applications in many areas. 
Finding the 
nearest neighbours means finding the nearest points to some reference point defined 
by some distance metric in some space. For example, e-Commerce websites might want 
to find the nearest neighbours of a product to recommend similar products to 
consumers for its recommender system, and financial institutions might want to find 
nearest neighbours of an account that performs fraudulent activities to identify 
similarly suspicious accounts. Some areas in computer science which the nearest 
neighbour problem is important include databases and data mining 
\cite{berkhin2006survey}, information retrieval \cite{croft2010search}, image and 
video databases \cite{faloutsos1994efficient,flickner1995query}, and machine 
learning \cite{weinberger2006distance}. \\

If the data set is of size $n$, and each data point is of dimension $d$, a naive 
approach would involve at looking at all the points in the entire data set and 
finding the nearest neighbours, a computation that has runtime of $\Theta(nd)$. 
Using data structures from computation geometry, we can reduce this to 
$\Theta(d\log(n))$, but the space complexity is $n^{O(d)}$, which is infeasible for 
high-dimensional data \cite{shalev2014understanding}. To avoid this ``curse of 
dimensionality", we can look toward approximate nearest neighbours, which is 
guaranteed to be within some distance bound of the true nearest neighbour 
(mathematically defined in next section). Some ways to tackle the approximate 
nearest neighbour problem include kd trees, balltrees, and locality sensitive 
hashing (LSH) \cite{shakhnarovich2006nearest}. \\

Amidst the decreasing storage cost and the increasing popularization of cloud 
computing and big dat\textbf{a, }we can no longer assume that the high dimensional feature 
vectors of the data set can fit into the memory of one machine. For example, the 80 
million tiny images data set for object and scene recognition is 240 GB, far larger 
than the available memory of a usual computer \cite{torralba200880}. Many 
organizations also store their data in distributed file systems, such as the Hadoop 
Distributed File System (HDFS) \cite{shvachko2010hadoop}. MapReduce 
\cite{dean2008mapreduce} and Apache Spark \cite{zaharia2010spark} have become the 
de-facto distributed computation frameworks for massive parallel computation on 
large data sets \cite{suri2011counting}. \\

Given the importance of finding nearest neighbours and the proliferation of Spark, 
it makes sense to investigate the application of Spark to the approximate nearest 
neighbour (ANN) problem. Specifically, we focus on LSH using Spark, since LSH seems 
to be less susceptible to the curse of dimensionality. We investigate the effect of 
parameters on the accuracy versus running time tradeoff, the scalability of the 
Spark implementation of LSH, and \red{a comparison of LSH to tree-based approaches 
for ANN}.

\section{Background and Related Work}

Distributed kd-trees \cite{aly2011distributed}

\begin{definition}[Nearest Neighbor Search]

	Given a  point set $P \in \mathbb{R}^{n \cdot d}$, for any query $q$, return 
	the $p \in P$ minimizing $\| p-q \|$, i.e. we wish to find the point closest to 
	$q$ by some metric.

\end{definition}

\begin{definition}[$k$-Nearest Neighbor Search]
	Given parameter $k$ and a point set $P \in \mathbb{R}^{n \cdot d}$, for any 
	query $q$, if any such exist, return the  $p \in P$ s.t. $\| p-q \| \leq r$, 
	i.e. we wish to find the closest k points.
\end{definition}

Some embedded literal typset code might 
look like the following :

{\tt \small
\begin{verbatim}
int wrap_fact(ClientData clientData,
              Tcl_Interp *interp,
              int argc, char *argv[]) {
    int result;
    int arg0;
    if (argc != 2) {
        interp->result = "wrong # args";
        return TCL_ERROR;
    }
    arg0 = atoi(argv[1]);
    result = fact(arg0);
    sprintf(interp->result,"%d",result);
    return TCL_OK;
}
\end{verbatim}
}

Now we're going to cite somebody.  Watch for the cite tag.
Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
in the source means a non-breaking space.  This way, your reference will
always be attached to the word that preceded it, instead of going to the
next line.

\section{Methodology}

Here's a typical figure reference.  The figure is centered at the
top of the column.  It's scaled.  It's explicitly placed.  You'll
have to tweak the numbers to get what you want.\\

% you can also use the wonderful epsfig package...
\begin{figure}[t]
\begin{center}
\begin{picture}(300,150)(0,200)
\put(-15,-30){\special{psfile = fig1.ps hscale = 50 vscale = 50}}
\end{picture}\\
\end{center}
\caption{Wonderful Flowchart}
\end{figure}

This text came after the figure, so we'll casually refer to Figure 1
as we go on our merry way.

It can get tricky typesetting Tcl and C code in LaTeX because they share
a lot of mystical feelings about certain magic characters.  You
will have to do a lot of escaping to typeset curly braces and percent
signs, for example, like this:
``The {\tt \%module} directive
sets the name of the initialization function.  This is optional, but is
recommended if building a Tcl 7.5 module.
Everything inside the {\tt \%\{, \%\}}
block is copied directly into the output. allowing the inclusion of
header files and additional C code." \\

Sometimes you want to really call attention to a piece of text.  You
can center it in the column like this:
\begin{center}
{\tt \_1008e614\_Vector\_p}
\end{center}
and people will really notice it.\\

\noindent
The noindent at the start of this paragraph makes it clear that it's
a continuation of the preceding text, not a new para in its own right.

Now this is an ingenious way to get a forced space.
{\tt Real~$*$} and {\tt double~$*$} are equivalent. 

Now here is another way to call attention to a line of code, but instead
of centering it, we noindent and bold it.\\

\noindent
{\bf \tt size\_t : fread ptr size nobj stream } \\

And here we have made an indented para like a definition tag (dt)
in HTML.  You don't need a surrounding list macro pair.
\begin{itemize}
\item[]  {\tt fread} reads from {\tt stream} into the array {\tt ptr} at
most {\tt nobj} objects of size {\tt size}.   {\tt fread} returns
the number of objects read. 
\end{itemize}
This concludes the definitions tag.

\section{Design}

\subsection{Locality Sensitive Hashing}
Locality-sensitive hashing (abbreviated LSH) is a method often used for answering approximate nearest neighbour queries in high-dimensional data sets. According to varity of the hash function design, LSH has many implementation. One of the most popular approach is random project(RP). The main idea of RP is .... \cite{wang2014hashing}  


\subsubsection{Random Projection into Hamming  Space}
\begin{definition}[Locality-sensitive]
	A hash function set $H = {h : S \rightarrow U}$ is called $(r_1, r_2, p_1, p_2)-$sensitive for certain distance measure $d(\cdot)$ if for any $v, q \in S$
	\begin{itemize}
		\item if $d(v, q)<r_1$, then $Pr_H[h(v)=h(q)]\geq p_1$
		\item if $d(v, q)>r_2$, then $Pr_H[h(v)=h(q)]\leq p_2$		
	\end{itemize}
\end{definition}
Originally, LSH embeds data space into the Hamming space. I

\begin{align*}
g(v) &= h_{a_1, a_2, ..., a_K}(v)  \\
&= [\sign{(a_1\cdot v )}, \sign{(a_1\cdot v )}, ..., \sign{(a_K\cdot v )}]
\end{align*}

\subsubsection{Random Projection into  Euclidean Space}

\begin{definition}[p-stable distribution]
	A distribution D is callled p-stable if there exists $p \geq 0$ such that for any n numbers $v_1, v_2,...,v_n$ and i.i.d. variables $X_1, X_2,...,X_n$ with distribution D, the random variable $\sum_i{v_i X_i} $ and $ (\sum_i |v_i|^p)^{1/p}X$ have the same distribution.


\end{definition}
According to \cite{zolotarev1986one}, stable distribution exist for any $p \in (0, 2]$. In particular:
\begin{itemize}	
	\item a Cauchy distribution with density function $c(x) = \frac{1}{\pi}\frac{1}{1+x^2}$, is 1-stable
	\item a Gaussian (normal) distribution with density function $g(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$, is 2-stable
\end{itemize}

First, we projects to a value on the real line. Next, we split line into equal-width segments of size r for buckets. Then we project the data point on the line. It is not hard to understand that the closer two points in the data space, the higher the probability that they project to the same segment. Through the discussion of p-stable distribution above, we know dot product can approximate the length of $\Vert v\Vert_p$. So the dot product keeps the property to hash into the same locality sensitivity.

The paper \cite{datar2004locality} proposes a hash function set:



\begin{align*}
g(v) = h_{a,b,w}(v) = \floor{\frac{(a\cdot v + b)}{w} }
\end{align*}

where $b \in (0, w)$ is a random number and $w$ is the length of line segment, or bucket length.

\subsubsection{LSH algorithm for k-Nearest Neighbor search for Classification}
The psudocode of applying distributed KNN algorithm to classification tasks can be viewed in \ref{classification}.

\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{Function LSHkNN} $(H, Q, k)$\;
	\Input{Hash Table $H$, Query Data Set $Q$, and Neighbor Number $k$}
	\Output{Prediction List of labels $L$}
	$L \leftarrow \infty$
	
	\For(\tcp*[f]{for each query sample}){$q=1, 2, \ldots, n$ }{
		\For(\tcp*[f]{for each hash table}){$i=1, 2, \ldots, m$}{        
			compute $h_{q, i} = \mathsf{g}_i(Q_q)$ \\
			find candidates $C_i = findHash(H, h_{q, i})$
		}
		collect candidates C = $\bigcup\limits_{i=1}^{m} C_{i}$ \\

		\eIf{$length(C)\neq 0$}
		{
			find best candidates by some distance matric $C^{\prime} = findTop(C, min(k, length(C)))$ \\
			get the majority $l_q = \arg \max\limits_{c \in C^{\prime}} freq(label_c)$
			c
		}
		{
			$L.insert(lable_{random})$
		}
	}
	return $L$\;
		
	\caption{LSH-based Approximate k-Nearest Neighbor Search Algorithm}
	\label{classification}
\end{algorithm}
\subsection{Distributed LSH through Spark}

The psudocode of our distributed KNN algorithm implementation can be viewed in \ref{knn}.


\begin{algorithm}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\underline{Function largeLSH} $(H, Q, k, d)$\;
	\Input{Hash Table $H$, Query Data Set $Q$, Neighbor Number $k$, and distance threshold $d$}
	\Output{Prediction List of labels $L$}
	generate ID column for $H$ and $Q$ \\
	results $\leftarrow$ H.approxSimilarityJoin(Q)  \tcp*[f]{search the hash table and union the results through a map-reduce way} \\
	resultsThres $\leftarrow$ filter(results, $d$) \tcp*[f]{get rid of candidates far from the query points}\\
	resultsSelected $\leftarrow$ \texttt{SELECT trainID, testID, distance FROM results\_thres} \\
	resultsPartitioned $\leftarrow$  findTopkByPartition(resultsSelected, $k$) \\
	$L$ $\leftarrow$ resultsPartitioned.map(groundtruthVector Intersect predictionVector)
	
	return $L$\;

	\caption{Distributed LSH}
	\label{knn}
\end{algorithm}

\section{Experimental Setup}

\subsection{Data}
\subsubsection{MNIST}
The MNIST database\footnote{\url{http://yann.lecun.com/exdb/mnist/}} of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. 

\subsubsection{SVHN}
SVHN\footnote{\url{http://ufldl.stanford.edu/housenumbers/}} is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). 

\subsubsection{SIFT}
\cite{jegou2011product}

\subsection{Task}
\subsubsection{Nearest Neighbor Search}
\subsubsection{Image Classification}
\subsection{Parameters}
\subsubsection{Hardware Configuration}
We use ..

\subsubsection{Parameters for LSH}
\subsubsection{Parameters for ANN-based Classification}

\section{Results}

\section{Summary}

You have to run {\tt latex} once to prepare your references for
munging.  Then run {\tt bibtex} to build your bibliography metadata.
Then run {\tt latex} twice to ensure all references have been resolved.
If your source file is called {\tt usenixTemplate.tex} and your {\tt
  bibtex} file is called {\tt usenixTemplate.bib}, here's what you do:
{\tt \small
\begin{verbatim}
latex usenixTemplate
bibtex usenixTemplate
latex usenixTemplate
latex usenixTemplate
\end{verbatim}
}

\section{Conclusion}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work. 

\begin{center}
{\tt ftp.site.dom/pub/myname/Wonderful}\\
\end{center}

Also, it's even greater when you can write that information is also 
available on the Wonderful homepage at 

\begin{center}
{\tt http://www.site.dom/\~{}myname/SWIG}
\end{center}

Now we get serious and fill in those references.  Remember you will
have to run latex twice on the document in order to resolve those
cite tags you met earlier.  This is where they get resolved.
We've preserved some real ones in addition to the template-speak.
After the bibliography you are DONE.

{\footnotesize \bibliographystyle{acm}
\bibliography{bibliography.bib}}


\theendnotes

\end{document}







